#!/usr/bin/env python3
"""
æ‰‹å¯«ResNetæ¶æ§‹å¯¦ç¾
å®Œå…¨é€æ˜çš„ResNeté‚è¼¯ï¼Œæ¯ä¸€å€‹æ®˜å·®å¡Šéƒ½æ¸…æ™°å¯è¦‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms
import argparse
import os
import time
import copy
import matplotlib.pyplot as plt
import numpy as np

class BasicBlock(nn.Module):
    """
    ResNetåŸºç¤æ®˜å·®å¡Š
    åŒ…å«å…©å€‹3x3å·ç©å±¤å’Œä¸€å€‹è·³èºé€£æ¥
    """
    expansion = 1  # é€šé“æ•¸æ“´å±•ä¿‚æ•¸
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        
        # ç¬¬ä¸€å€‹å·ç©å±¤
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # ç¬¬äºŒå€‹å·ç©å±¤
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³èºé€£æ¥çš„ä¸‹æ¡æ¨£å±¤ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.downsample = downsample
        self.stride = stride
    
    def forward(self, x):
        # ä¿å­˜è¼¸å…¥ç”¨æ–¼è·³èºé€£æ¥
        identity = x
        
        # ç¬¬ä¸€å€‹å·ç©-BN-ReLU
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        
        # ç¬¬äºŒå€‹å·ç©-BN
        out = self.conv2(out)
        out = self.bn2(out)
        
        # è·³èºé€£æ¥ï¼šå¦‚æœç¶­åº¦ä¸åŒ¹é…éœ€è¦ä¸‹æ¡æ¨£
        if self.downsample is not None:
            identity = self.downsample(x)
        
        # æ®˜å·®é€£æ¥ï¼šF(x) + x
        out += identity
        out = F.relu(out)
        
        return out

class HandwrittenResNet(nn.Module):
    """
    æ‰‹å¯«çš„ResNetæ¶æ§‹
    æ¸…æ™°å±•ç¤ºæ¯ä¸€å±¤çš„æ§‹é€ é‚è¼¯
    """
    
    def __init__(self, block, layers, num_classes=2, zero_init_residual=False):
        super(HandwrittenResNet, self).__init__()
        
        print("ğŸ—ï¸ æ§‹å»ºæ‰‹å¯«ResNetæ¶æ§‹...")
        
        self.in_channels = 64
        
        # ç¬¬ä¸€å±¤ï¼š7x7å·ç© + BatchNorm + ReLU + MaxPool
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # å››å€‹æ®˜å·®å±¤çµ„
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†é¡å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
        # æ¬Šé‡åˆå§‹åŒ–
        self._initialize_weights(zero_init_residual)
        
        print("âœ… æ‰‹å¯«ResNetæ¶æ§‹æ§‹å»ºå®Œæˆ")
        self._print_architecture_info(block, layers)
    
    def _make_layer(self, block, channels, blocks, stride=1):
        """
        æ§‹å»ºä¸€å€‹æ®˜å·®å±¤çµ„
        block: BasicBlocké¡
        channels: è¼¸å‡ºé€šé“æ•¸
        blocks: è©²å±¤çµ„ä¸­çš„blockæ•¸é‡
        stride: ç¬¬ä¸€å€‹blockçš„æ­¥é•·
        """
        downsample = None
        
        # å¦‚æœæ­¥é•·ä¸ç‚º1æˆ–è€…è¼¸å…¥è¼¸å‡ºé€šé“æ•¸ä¸åŒ¹é…ï¼Œéœ€è¦ä¸‹æ¡æ¨£
        if stride != 1 or self.in_channels != channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, channels * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(channels * block.expansion),
            )
        
        layers = []
        # ç¬¬ä¸€å€‹blockå¯èƒ½éœ€è¦ä¸‹æ¡æ¨£
        layers.append(block(self.in_channels, channels, stride, downsample))
        self.in_channels = channels * block.expansion
        
        # å¾ŒçºŒblockä¿æŒç›¸åŒç¶­åº¦
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, channels))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self, zero_init_residual):
        """æ¬Šé‡åˆå§‹åŒ–"""
        print("ğŸ² åˆå§‹åŒ–ResNetæ¬Šé‡...")
        
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
        
        # Zero-initialize the last BN in each residual branch
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)
    
    def _print_architecture_info(self, block, layers):
        """æ‰“å°æ¶æ§‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š ResNetæ¶æ§‹ä¿¡æ¯:")
        print(f"   å±¤é…ç½®: {layers}")
        print(f"   ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
        print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")
        
        # è¨ˆç®—æ¯å±¤çš„è¼¸å‡ºå°ºå¯¸
        print(f"ğŸ“ å„å±¤è¼¸å‡ºå°ºå¯¸:")
        print(f"   è¼¸å…¥åœ–ç‰‡: (3, 224, 224)")
        print(f"   conv1 + pool: (64, 56, 56)")
        print(f"   layer1: ({64 * block.expansion}, 56, 56)")
        print(f"   layer2: ({128 * block.expansion}, 28, 28)")
        print(f"   layer3: ({256 * block.expansion}, 14, 14)")
        print(f"   layer4: ({512 * block.expansion}, 7, 7)")
        print(f"   avgpool: ({512 * block.expansion}, 1, 1)")
        print(f"   fc: (2,)")
    
    def forward(self, x):
        """å‰å‘å‚³æ’­ - å±•ç¤ºå®Œæ•´çš„æ•¸æ“šæµ"""
        # è¼¸å…¥: (batch_size, 3, 224, 224)
        
        # åˆå§‹å·ç©å±¤
        x = self.conv1(x)    # (batch_size, 64, 112, 112)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)  # (batch_size, 64, 56, 56)
        
        # å››å€‹æ®˜å·®å±¤çµ„
        x = self.layer1(x)   # (batch_size, 64, 56, 56)
        x = self.layer2(x)   # (batch_size, 128, 28, 28)
        x = self.layer3(x)   # (batch_size, 256, 14, 14)
        x = self.layer4(x)   # (batch_size, 512, 7, 7)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.avgpool(x)  # (batch_size, 512, 1, 1)
        x = x.view(x.size(0), -1)  # (batch_size, 512)
        
        # åˆ†é¡å™¨
        x = self.fc(x)       # (batch_size, 2)
        
        return x

def resnet18(num_classes=2, **kwargs):
    """æ§‹å»ºResNet-18"""
    return HandwrittenResNet(BasicBlock, [2, 2, 2, 2], num_classes, **kwargs)

def resnet34(num_classes=2, **kwargs):
    """æ§‹å»ºResNet-34"""
    return HandwrittenResNet(BasicBlock, [3, 4, 6, 3], num_classes, **kwargs)

class HandwrittenResNetTrainer:
    """æ‰‹å¯«ResNetè¨“ç·´å™¨"""
    
    def __init__(self, data_dir, architecture='resnet34', target_accuracy=1.0):
        self.data_dir = data_dir
        self.architecture = architecture
        self.target_accuracy = target_accuracy
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ¯ ç›®æ¨™è¨“ç·´æº–ç¢ºç‡: {target_accuracy*100:.1f}%")
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {self.device}")
        print(f"ğŸ—ï¸ æ¶æ§‹: æ‰‹å¯«{architecture.upper()}")
        
        # æ•¸æ“šé è™•ç†
        self.data_transforms = {
            'train': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }
        
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = nn.CrossEntropyLoss()
        self.dataloaders = {}
        self.dataset_sizes = {}
        self.class_names = []
    
    def load_data(self):
        """åŠ è¼‰æ•¸æ“š"""
        print("ğŸ“‚ æ­£åœ¨åŠ è¼‰æ•¸æ“š...")
        
        image_datasets = {x: datasets.ImageFolder(os.path.join(self.data_dir, x),
                                                self.data_transforms[x])
                         for x in ['train', 'val']}
        
        self.dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,
                                        shuffle=(x == 'train'), num_workers=4)
                          for x in ['train', 'val']}
        
        self.dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
        self.class_names = image_datasets['train'].classes
        
        print(f"âœ… è¨“ç·´é›†å¤§å°: {self.dataset_sizes['train']}")
        print(f"âœ… é©—è­‰é›†å¤§å°: {self.dataset_sizes['val']}")
        print(f"âœ… é¡åˆ¥: {self.class_names}")
    
    def build_model(self):
        """æ§‹å»ºæ‰‹å¯«ResNetæ¨¡å‹"""
        print(f"ğŸ—ï¸ æ§‹å»ºæ‰‹å¯«{self.architecture.upper()}...")
        
        if self.architecture == 'resnet18':
            self.model = resnet18(num_classes=len(self.class_names))
        elif self.architecture == 'resnet34':
            self.model = resnet34(num_classes=len(self.class_names))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¶æ§‹: {self.architecture}")
        
        self.model = self.model.to(self.device)
        
        # å„ªåŒ–å™¨è¨­ç½®
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=60, gamma=0.1)
        
        print("âœ… æ‰‹å¯«ResNetæ¨¡å‹æ§‹å»ºå®Œæˆ")
    
    def train_to_perfection(self, max_epochs=250):
        """è¨“ç·´åˆ°ç›®æ¨™æº–ç¢ºç‡"""
        print(f"ğŸš€ é–‹å§‹è¨“ç·´æ‰‹å¯«ResNetåˆ° {self.target_accuracy*100:.1f}% æº–ç¢ºç‡...")
        print("=" * 60)
        
        since = time.time()
        best_model_wts = copy.deepcopy(self.model.state_dict())
        best_acc = 0.0
        
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        for epoch in range(max_epochs):
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f'Epoch {epoch+1}/{max_epochs} (LR: {current_lr:.6f})')
            print('-' * 50)
            
            epoch_train_acc = 0.0
            
            # è¨“ç·´å’Œé©—è­‰éšæ®µ
            for phase in ['train', 'val']:
                if phase == 'train':
                    self.model.train()
                else:
                    self.model.eval()
                
                running_loss = 0.0
                running_corrects = 0
                
                for inputs, labels in self.dataloaders[phase]:
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    
                    self.optimizer.zero_grad()
                    
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = self.model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = self.criterion(outputs, labels)
                        
                        if phase == 'train':
                            loss.backward()
                            # æ¢¯åº¦è£å‰ª
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                            self.optimizer.step()
                    
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                
                epoch_loss = running_loss / self.dataset_sizes[phase]
                epoch_acc = running_corrects.double() / self.dataset_sizes[phase]
                
                print(f'{phase} Loss: {epoch_loss:.6f} Acc: {epoch_acc:.4f} ({epoch_acc*100:.2f}%)')
                
                if phase == 'train':
                    train_losses.append(epoch_loss)
                    train_accuracies.append(epoch_acc.cpu().numpy())
                    epoch_train_acc = epoch_acc
                    self.scheduler.step()
                else:
                    val_losses.append(epoch_loss)
                    val_accuracies.append(epoch_acc.cpu().numpy())
                    
                    if epoch_acc > best_acc:
                        best_acc = epoch_acc
                        best_model_wts = copy.deepcopy(self.model.state_dict())
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
            if epoch_train_acc >= self.target_accuracy:
                print(f"\nğŸ‰ é”åˆ°ç›®æ¨™è¨“ç·´æº–ç¢ºç‡ {self.target_accuracy*100:.1f}%ï¼")
                print(f"å¯¦éš›è¨“ç·´æº–ç¢ºç‡: {epoch_train_acc*100:.2f}%")
                print(f"åœ¨ç¬¬ {epoch+1} è¼ªé”æˆç›®æ¨™")
                break
            
            # é€²åº¦å ±å‘Š
            if (epoch + 1) % 25 == 0:
                elapsed = time.time() - since
                print(f"ğŸ“Š é€²åº¦å ±å‘Š: è¨“ç·´æº–ç¢ºç‡ {epoch_train_acc*100:.2f}%, è€—æ™‚ {elapsed/60:.1f}åˆ†é˜")
            
            print()
        
        time_elapsed = time.time() - since
        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'Best val Acc: {best_acc:4f}')
        
        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(best_model_wts)
        
        # ç¹ªè£½è¨“ç·´æ›²ç·š
        self.plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)
        
        return self.model
    
    def plot_training_curves(self, train_losses, val_losses, train_accs, val_accs):
        """ç¹ªåˆ¶è¨“ç·´æ›²ç·š"""
        plt.figure(figsize=(16, 12))
        
        # Lossæ›²ç·š
        plt.subplot(2, 3, 1)
        plt.plot(train_losses, label='Train Loss', color='blue', linewidth=2)
        plt.plot(val_losses, label='Val Loss', color='red', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        plt.grid(True)
        plt.yscale('log')
        
        # æº–ç¢ºç‡æ›²ç·š
        plt.subplot(2, 3, 2)
        plt.plot(train_accs, label='Train Acc', color='blue', linewidth=2)
        plt.plot(val_accs, label='Val Acc', color='red', linewidth=2)
        plt.axhline(y=self.target_accuracy, color='green', linestyle='--', 
                   label=f'Target ({self.target_accuracy*100:.0f}%)')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy')
        plt.legend()
        plt.grid(True)
        
        # æ”¾å¤§è¨“ç·´æº–ç¢ºç‡
        plt.subplot(2, 3, 3)
        plt.plot(train_accs, label='Train Acc', color='blue', linewidth=2, marker='o', markersize=2)
        plt.axhline(y=self.target_accuracy, color='green', linestyle='--',
                   label=f'Target ({self.target_accuracy*100:.0f}%)')
        plt.xlabel('Epoch')
        plt.ylabel('Training Accuracy')
        plt.title('Training Accuracy (Detailed)')
        if max(train_accs) > 0.8:
            plt.ylim(0.7, 1.01)
        plt.legend()
        plt.grid(True)
        
        # éæ“¬åˆæŒ‡æ¨™
        plt.subplot(2, 3, 4)
        overfitting = np.array(train_accs) - np.array(val_accs)
        plt.plot(overfitting, color='purple', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Train Acc - Val Acc')
        plt.title('Overfitting Indicator')
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.grid(True)
        
        # æœ€è¿‘50è¼ªçš„æº–ç¢ºç‡
        plt.subplot(2, 3, 5)
        recent_epochs = min(50, len(train_accs))
        recent_accs = train_accs[-recent_epochs:]
        plt.plot(range(len(train_accs)-recent_epochs, len(train_accs)), recent_accs, 
                color='blue', linewidth=2, marker='o', markersize=3)
        plt.axhline(y=self.target_accuracy, color='green', linestyle='--',
                   label=f'Target ({self.target_accuracy*100:.0f}%)')
        plt.xlabel('Epoch')
        plt.ylabel('Training Accuracy')
        plt.title(f'Recent {recent_epochs} Epochs')
        plt.legend()
        plt.grid(True)
        
        # æ¨¡å‹æ¶æ§‹ä¿¡æ¯
        plt.subplot(2, 3, 6)
        plt.text(0.1, 0.8, f'Architecture: {self.architecture.upper()}', fontsize=12, fontweight='bold')
        plt.text(0.1, 0.7, f'Target Accuracy: {self.target_accuracy*100:.1f}%', fontsize=11)
        plt.text(0.1, 0.6, f'Final Train Acc: {train_accs[-1]*100:.2f}%', fontsize=11)
        plt.text(0.1, 0.5, f'Final Val Acc: {val_accs[-1]*100:.2f}%', fontsize=11)
        plt.text(0.1, 0.4, f'Total Epochs: {len(train_accs)}', fontsize=11)
        plt.text(0.1, 0.3, f'Best Val Acc: {max(val_accs)*100:.2f}%', fontsize=11)
        plt.text(0.1, 0.2, f'Training Mode: From Scratch', fontsize=11, style='italic')
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.axis('off')
        plt.title('Training Summary')
        
        plt.tight_layout()
        plt.savefig('handwritten_resnet_training_curves.png', dpi=300, bbox_inches='tight')
        print(f"âœ… è¨“ç·´æ›²ç·šå·²ä¿å­˜: handwritten_resnet_training_curves.png")
        plt.show()
    
    def save_model(self, filepath='best_cat_dog_model.pth'):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'class_names': self.class_names,
            'model_architecture': f'Handwritten{self.architecture.upper()}',
            'target_accuracy': self.target_accuracy,
            'training_type': 'best_cat_dog_model'
        }, filepath)
        print(f"ğŸ¯ æ‰‹å¯«ResNetæ¨¡å‹å·²ä¿å­˜: {filepath}")

def main():
    parser = argparse.ArgumentParser(description='æ‰‹å¯«ResNetå¾é›¶é–‹å§‹è¨“ç·´')
    parser.add_argument('--data-dir', type=str, default='file/kaggle_cats_vs_dogs_f',
                       help='æ•¸æ“šé›†è·¯å¾‘')
    parser.add_argument('--architecture', type=str, default='resnet18',
                       choices=['resnet18', 'resnet34'],
                       help='ResNetæ¶æ§‹')
    parser.add_argument('--target-accuracy', type=float, default=1.0,
                       help='ç›®æ¨™è¨“ç·´æº–ç¢ºç‡ (0.0-1.0)')
    parser.add_argument('--max-epochs', type=int, default=250,
                       help='æœ€å¤§è¨“ç·´è¼ªæ•¸')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šè·¯å¾‘: {args.data_dir}")
        return
    
    print("ğŸ¯ æ‰‹å¯«ResNetå¾é›¶é–‹å§‹è¨“ç·´å™¨")
    print("=" * 50)
    print(f"ğŸ“‚ æ•¸æ“šè·¯å¾‘: {args.data_dir}")
    print(f"ğŸ—ï¸ æ¨¡å‹æ¶æ§‹: æ‰‹å¯«{args.architecture.upper()}")
    print(f"ğŸ¯ ç›®æ¨™æº–ç¢ºç‡: {args.target_accuracy*100:.1f}%")
    print(f"ğŸ”„ æœ€å¤§è¼ªæ•¸: {args.max_epochs}")
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = HandwrittenResNetTrainer(args.data_dir, args.architecture, args.target_accuracy)
    
    # è¨“ç·´æµç¨‹
    trainer.load_data()
    trainer.build_model()
    trainer.train_to_perfection(args.max_epochs)
    trainer.save_model('best_cat_dog_model.pth')
    
    print("\nğŸ‰ æ‰‹å¯«ResNetè¨“ç·´å®Œæˆï¼")
    print("\nğŸ“‹ æ¥ä¸‹ä¾†ä½ å¯ä»¥:")
    print("1. python predict.py --model best_cat_dog_model.pth --evaluate-train")
    print("2. é©—è­‰æ˜¯å¦é”åˆ° 100% è¨“ç·´æº–ç¢ºç‡")


main()
