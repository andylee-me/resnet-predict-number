#!/usr/bin/env python3
"""
è‡ªå®šç¾©CNNæ¶æ§‹å¾é›¶é–‹å§‹è¨“ç·´è²“ç‹—åˆ†é¡å™¨
æ¯ä¸€å±¤éƒ½æ¸…æ™°å¯è¦‹ï¼Œå®Œå…¨è‡ªä¸»è¨­è¨ˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms
import argparse
import os
import time
import copy
import matplotlib.pyplot as plt
import numpy as np

class CustomCNN(nn.Module):
    """
    è‡ªå®šç¾©CNNæ¶æ§‹
    æ¸…æ™°çš„å±¤æ¬¡çµæ§‹ï¼Œæ¯ä¸€å±¤éƒ½æ˜ç¢ºå®šç¾©
    """
    def __init__(self, num_classes=2):
        super(CustomCNN, self).__init__()
        
        print("ğŸ—ï¸ æ§‹å»ºè‡ªå®šç¾©CNNæ¶æ§‹...")
        
        # ç¬¬ä¸€å€‹å·ç©å¡Š - å­¸ç¿’åŸºæœ¬é‚Šç·£å’Œç´‹ç†
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 224x224 -> 112x112
        )
        
        # ç¬¬äºŒå€‹å·ç©å¡Š - å­¸ç¿’æ›´è¤‡é›œçš„å½¢ç‹€
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 112x112 -> 56x56
        )
        
        # ç¬¬ä¸‰å€‹å·ç©å¡Š - å­¸ç¿’ç‰¹å¾µçµ„åˆ
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 56x56 -> 28x28
        )
        
        # ç¬¬å››å€‹å·ç©å¡Š - å­¸ç¿’é«˜ç´šç‰¹å¾µ
        self.conv_block4 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14
        )
        
        # ç¬¬äº”å€‹å·ç©å¡Š - å­¸ç¿’æŠ½è±¡ç‰¹å¾µ
        self.conv_block5 = nn.Sequential(
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7
        )
        
        # å…¨å±€å¹³å‡æ± åŒ– - æ›¿ä»£flattenï¼Œæ›´å„ªé›…
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # 7x7 -> 1x1
        
        # åˆ†é¡é ­ - æœ€çµ‚æ±ºç­–å±¤
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
        
        print("âœ… è‡ªå®šç¾©CNNæ¶æ§‹æ§‹å»ºå®Œæˆ")
        self._print_model_info()
    
    def _initialize_weights(self):
        """è‡ªå®šç¾©æ¬Šé‡åˆå§‹åŒ–"""
        print("ğŸ² åˆå§‹åŒ–ç¶²çµ¡æ¬Šé‡...")
        
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                # å·ç©å±¤ä½¿ç”¨Heåˆå§‹åŒ–ï¼ˆé©åˆReLUï¼‰
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                # BNå±¤åˆå§‹åŒ–
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # å…¨é€£æ¥å±¤ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
        print(f"   ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
        print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")
    
    def forward(self, x):
        """å‰å‘å‚³æ’­ - æ¸…æ™°çš„æ•¸æ“šæµ"""
        # è¼¸å…¥: (batch_size, 3, 224, 224)
        
        # ç‰¹å¾µæå–éšæ®µ
        x = self.conv_block1(x)  # (batch_size, 32, 112, 112)
        x = self.conv_block2(x)  # (batch_size, 64, 56, 56)
        x = self.conv_block3(x)  # (batch_size, 128, 28, 28)
        x = self.conv_block4(x)  # (batch_size, 256, 14, 14)
        x = self.conv_block5(x)  # (batch_size, 512, 7, 7)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.global_avg_pool(x)  # (batch_size, 512, 1, 1)
        x = x.view(x.size(0), -1)    # (batch_size, 512)
        
        # åˆ†é¡éšæ®µ
        x = self.classifier(x)       # (batch_size, num_classes)
        
        return x

class CustomCNNTrainer:
    """è‡ªå®šç¾©CNNè¨“ç·´å™¨"""
    
    def __init__(self, data_dir, target_accuracy=1.0):
        self.data_dir = data_dir
        self.target_accuracy = target_accuracy
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ¯ ç›®æ¨™è¨“ç·´æº–ç¢ºç‡: {target_accuracy*100:.1f}%")
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {self.device}")
        print(f"ğŸ—ï¸ æ¶æ§‹: è‡ªå®šç¾©CNN (5å€‹å·ç©å¡Š)")
        
        # æ•¸æ“šé è™•ç†
        self.data_transforms = {
            'train': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
            'val': transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ]),
        }
        
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = nn.CrossEntropyLoss()
        self.dataloaders = {}
        self.dataset_sizes = {}
        self.class_names = []
    
    def load_data(self):
        """åŠ è¼‰æ•¸æ“š"""
        print("ğŸ“‚ æ­£åœ¨åŠ è¼‰æ•¸æ“š...")
        
        image_datasets = {x: datasets.ImageFolder(os.path.join(self.data_dir, x),
                                                self.data_transforms[x])
                         for x in ['train', 'val']}
        
        self.dataloaders = {x: DataLoader(image_datasets[x], batch_size=32,
                                        shuffle=(x == 'train'), num_workers=4)
                          for x in ['train', 'val']}
        
        self.dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
        self.class_names = image_datasets['train'].classes
        
        print(f"âœ… è¨“ç·´é›†å¤§å°: {self.dataset_sizes['train']}")
        print(f"âœ… é©—è­‰é›†å¤§å°: {self.dataset_sizes['val']}")
        print(f"âœ… é¡åˆ¥: {self.class_names}")
    
    def build_model(self):
        """æ§‹å»ºè‡ªå®šç¾©æ¨¡å‹"""
        print("ğŸ—ï¸ æ§‹å»ºè‡ªå®šç¾©CNNæ¨¡å‹...")
        
        self.model = CustomCNN(num_classes=len(self.class_names))
        self.model = self.model.to(self.device)
        
        # å„ªåŒ–å™¨è¨­ç½®
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=50, gamma=0.1)
        
        print("âœ… æ¨¡å‹æ§‹å»ºå®Œæˆ")
    
    def train_to_perfection(self, max_epochs=200):
        """è¨“ç·´åˆ°ç›®æ¨™æº–ç¢ºç‡"""
        print(f"ğŸš€ é–‹å§‹è¨“ç·´è‡ªå®šç¾©CNNåˆ° {self.target_accuracy*100:.1f}% æº–ç¢ºç‡...")
        print("=" * 60)
        
        since = time.time()
        best_model_wts = copy.deepcopy(self.model.state_dict())
        best_acc = 0.0
        
        train_losses = []
        val_losses = []
        train_accuracies = []
        val_accuracies = []
        
        for epoch in range(max_epochs):
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f'Epoch {epoch+1}/{max_epochs} (LR: {current_lr:.6f})')
            print('-' * 50)
            
            epoch_train_acc = 0.0
            
            # è¨“ç·´å’Œé©—è­‰éšæ®µ
            for phase in ['train', 'val']:
                if phase == 'train':
                    self.model.train()
                else:
                    self.model.eval()
                
                running_loss = 0.0
                running_corrects = 0
                
                for inputs, labels in self.dataloaders[phase]:
                    inputs = inputs.to(self.device)
                    labels = labels.to(self.device)
                    
                    self.optimizer.zero_grad()
                    
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = self.model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = self.criterion(outputs, labels)
                        
                        if phase == 'train':
                            loss.backward()
                            # æ¢¯åº¦è£å‰ª
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                            self.optimizer.step()
                    
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)
                
                epoch_loss = running_loss / self.dataset_sizes[phase]
                epoch_acc = running_corrects.double() / self.dataset_sizes[phase]
                
                print(f'{phase} Loss: {epoch_loss:.6f} Acc: {epoch_acc:.4f} ({epoch_acc*100:.2f}%)')
                
                if phase == 'train':
                    train_losses.append(epoch_loss)
                    train_accuracies.append(epoch_acc.cpu().numpy())
                    epoch_train_acc = epoch_acc
                    self.scheduler.step()
                else:
                    val_losses.append(epoch_loss)
                    val_accuracies.append(epoch_acc.cpu().numpy())
                    
                    if epoch_acc > best_acc:
                        best_acc = epoch_acc
                        best_model_wts = copy.deepcopy(self.model.state_dict())
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
            if epoch_train_acc >= self.target_accuracy:
                print(f"\nğŸ‰ é”åˆ°ç›®æ¨™è¨“ç·´æº–ç¢ºç‡ {self.target_accuracy*100:.1f}%ï¼")
                print(f"å¯¦éš›è¨“ç·´æº–ç¢ºç‡: {epoch_train_acc*100:.2f}%")
                print(f"åœ¨ç¬¬ {epoch+1} è¼ªé”æˆç›®æ¨™")
                break
            
            # é€²åº¦å ±å‘Š
            if (epoch + 1) % 20 == 0:
                elapsed = time.time() - since
                print(f"ğŸ“Š é€²åº¦å ±å‘Š: è¨“ç·´æº–ç¢ºç‡ {epoch_train_acc*100:.2f}%, è€—æ™‚ {elapsed/60:.1f}åˆ†é˜")
            
            print()
        
        time_elapsed = time.time() - since
        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
        print(f'Best val Acc: {best_acc:4f}')
        
        # è¼‰å…¥æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(best_model_wts)
        
        # ç¹ªè£½è¨“ç·´æ›²ç·š
        self.plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)
        
        return self.model
    
    def plot_training_curves(self, train_losses, val_losses, train_accs, val_accs):
        """ç¹ªåˆ¶è¨“ç·´æ›²ç·š"""
        plt.figure(figsize=(15, 10))
        
        # Lossæ›²ç·š
        plt.subplot(2, 2, 1)
        plt.plot(train_losses, label='Train Loss', color='blue', linewidth=2)
        plt.plot(val_losses, label='Val Loss', color='red', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss')
        plt.legend()
        plt.grid(True)
        
        # æº–ç¢ºç‡æ›²ç·š
        plt.subplot(2, 2, 2)
        plt.plot(train_accs, label='Train Acc', color='blue', linewidth=2)
        plt.plot(val_accs, label='Val Acc', color='red', linewidth=2)
        plt.axhline(y=self.target_accuracy, color='green', linestyle='--', 
                   label=f'Target ({self.target_accuracy*100:.0f}%)')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy')
        plt.legend()
        plt.grid(True)
        
        # æ”¾å¤§è¨“ç·´æº–ç¢ºç‡
        plt.subplot(2, 2, 3)
        plt.plot(train_accs, label='Train Acc', color='blue', linewidth=2, marker='o', markersize=3)
        plt.axhline(y=self.target_accuracy, color='green', linestyle='--',
                   label=f'Target ({self.target_accuracy*100:.0f}%)')
        plt.xlabel('Epoch')
        plt.ylabel('Training Accuracy')
        plt.title('Training Accuracy (Detailed)')
        if max(train_accs) > 0.8:
            plt.ylim(0.7, 1.01)
        plt.legend()
        plt.grid(True)
        
        # éæ“¬åˆæŒ‡æ¨™
        plt.subplot(2, 2, 4)
        overfitting = np.array(train_accs) - np.array(val_accs)
        plt.plot(overfitting, color='purple', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Train Acc - Val Acc')
        plt.title('Overfitting Indicator')
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('custom_cnn_training_curves.png', dpi=300, bbox_inches='tight')
        print(f"âœ… è¨“ç·´æ›²ç·šå·²ä¿å­˜: custom_cnn_training_curves.png")
        plt.show()
    
    def save_model(self, filepath='best_cat_dog_model.pth'):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'class_names': self.class_names,
            'model_architecture': 'CustomCNN',
            'target_accuracy': self.target_accuracy,
            'training_type': 'custom_cnn_from_scratch'
        }, filepath)
        print(f"ğŸ¯ è‡ªå®šç¾©CNNæ¨¡å‹å·²ä¿å­˜: {filepath}")

def main():
    parser = argparse.ArgumentParser(description='è‡ªå®šç¾©CNNå¾é›¶é–‹å§‹è¨“ç·´')
    parser.add_argument('--data-dir', type=str, default='file/kaggle_cats_vs_dogs_f',
                       help='æ•¸æ“šé›†è·¯å¾‘')
    parser.add_argument('--target-accuracy', type=float, default=1.0,
                       help='ç›®æ¨™è¨“ç·´æº–ç¢ºç‡ (0.0-1.0)')
    parser.add_argument('--max-epochs', type=int, default=200,
                       help='æœ€å¤§è¨“ç·´è¼ªæ•¸')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.data_dir):
        print(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šè·¯å¾‘: {args.data_dir}")
        return
    
    print("ğŸ¯ è‡ªå®šç¾©CNNå¾é›¶é–‹å§‹è¨“ç·´å™¨")
    print("=" * 50)
    print(f"ğŸ“‚ æ•¸æ“šè·¯å¾‘: {args.data_dir}")
    print(f"ğŸ—ï¸ æ¨¡å‹æ¶æ§‹: è‡ªå®šç¾©CNN")
    print(f"ğŸ¯ ç›®æ¨™æº–ç¢ºç‡: {args.target_accuracy*100:.1f}%")
    print(f"ğŸ”„ æœ€å¤§è¼ªæ•¸: {args.max_epochs}")
    
    # å‰µå»ºè¨“ç·´å™¨
    trainer = CustomCNNTrainer(args.data_dir, args.target_accuracy)
    
    # è¨“ç·´æµç¨‹
    trainer.load_data()
    trainer.build_model()
    trainer.train_to_perfection(args.max_epochs)
    trainer.save_model('custom_cnn_model.pth')
    
    print("\nğŸ‰ è‡ªå®šç¾©CNNè¨“ç·´å®Œæˆï¼")
    print("\nğŸ“‹ æ¥ä¸‹ä¾†ä½ å¯ä»¥:")
    print("1. python predict.py --model custom_cnn_model.pth --evaluate-train")
    print("2. é©—è­‰æ˜¯å¦é”åˆ° 100% è¨“ç·´æº–ç¢ºç‡")


main()
